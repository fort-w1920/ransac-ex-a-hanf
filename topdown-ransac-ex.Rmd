## (Ran-)Sackgang

Wir implementieren die [RANSAC-Methode](https://en.wikipedia.org/wiki/Random_sample_consensus) für lineare Modelle in einer Funktion `ransaclm` mit (mindestens) Argumenten `formula` (wie in `lm`), `data` (wie in `lm`), `error_threshold` (=$t$ im verlinkten Wikipedia-Artikel), `inlier_threshold` (=$d$ im verlinkten) und `iterations`.  
Was für zusätzliche Argumente benötigt die Funktion? (bitte erst selbst überlegen, dann Codebeispiel weiter unten anschauen...)  

Die Funktion soll eine Liste mit Einträgen `model` und `data` zurückliefern:

- `model`: das gefundene beste Modell (ein `lm`-Objekt). `NULL` falls kein Modell gefunden wurde was den gegebenen thresholds entspricht.
- `data`: die für die Suche nach `model` verwendeten Daten, im Erfolgsfall ergänzt um eine `logical`-Spalte `.consensus_set` die das gefundene beste `consensus set` für `model` definiert.

--------------

a) *First, understand the problem. Then, write the code.* Skizzieren Sie zunächst in Pseudo-Code wie der RANSAC Algorithmus in Einzelschritte und Sub-Funktionen zergliedert werden kann. Definieren Sie sauber was jeweils die Inputs und Outputs dieser Sub-Funktionen sein sollen. Gehen Sie hier iterativ vor -- verfeinern Sie sukzessive die grobe Untergliederung in kleinere Teilschritte. 

Mein erster Entwurf sah wie folgt aus:
```
ransaclm <- function(formula, data, error_threshold, inlier_threshold, iterations=100, seed) {
  init_ransac()
  for (i in seq_len(iterations)) {
    current_sample <- select_inliers()
    current_model <- find_model()
    inliers <- find_inliers()
    if (many_inliers()) {
      better_model <- find_model()
      current_error <- extract_error()
      if (obtained_better_fit()) {
        save_new_model()
      }
    } 
  }
}
```
In der `init_ransac`-Funktion würden wir hier diverse Variablen deklarieren, die im späteren Verlauf gebraucht werden. Dann wiederholen wir `iterations`-mal:
- `select_inliers`, um zufällig eine passende Anzahl an Beobachtungen zu ziehen
- mit diesen kann dann ein Modell gefittet werden (`find_model()`)
- anschließend werden weitere Inlier gefunden, die von diesem Modell gut gefittet werden
- falls es viele Inlier gibt, dann fitten wir ein neues Modell und schauen, ob es den overall fit verbessert.

- Denken Sie defensiv: Was könnte schiefgehen, wie gehen Sie sinnvoll damit um? Stichworte: Untaugliche Argumente, Daten mit fehlenden Werten, Faktorvariablen, lineare Modelle mit "$p > n$", etc....

Grundsätzlich können wir die Datentypen der Inputs checken. Außerdem sollte der Dataframe nur für die lm-Funktion passende Datentypen beinhalten und keine fehlenden Werte aufweisen. Die übergebene Formel sollte natürlich nur Kovariablen enthalten, die auch im Dataframe vorkommen. Um sicherzustellen, dass das lineare Modell nicht unterspezifiziert ist, brauchen wir mehr Beobachtungen als Kovariablen. All diese Dinge können über Assertions abgesichert werden.

Das reicht aber noch nicht, denn durch das Sampling kann es passieren, dass wir z. B. mehrfach vorkommende Beobachtungen aus dem Dataframe samplen. Falls wir Pech haben, bekommen wir so wieder ein unterspezifiziertes Modell. Da dies viele verschiedene Gründe haben kann, die nicht einfach im Vorfeld zu testen sind, macht hier ein tryCatch-Block Sinn.

- Denken Sie parallel: Wie können Sie diesen Algorithmus am besten (plattformunabhängig) parallelisieren? 
(Welches Paket -- `{future.apply}`, `{foreach}` + `{do??}`, `{parallel}` -- Sie für die Parallelisierung benutzen bleibt Ihnen überlassen.)

Das wesentliche Problem war hier die Speicherung des "besten" Modells. Im sequentiellen Fall kann man einfach nach jeder Iteration checken, ob der Error kleiner geworden ist. Ich glaube das geht parallelisiert nicht (das aktuell beste Modell kann sich ändern, während ein Thread diesen Check macht). Ich habe daher einfach eine Liste mit den Ergebnissen aus jeder Iteration zurückgegeben und hinterher geschaut, bei welchem Modell der Error minimal war - das ist vergleichsweise sehr ineffizient (gegebenenfalls speichert man sehr viele, evtl. auch große Modelle) - man könnte vielleicht das beste Modell pro Core speichern und das dann am Schluss vergleichen, aber ich wusste nicht wie / ob das geht. Ich denke das würde bei mir sämtliche Laufzeitvorteile aus der Parallelisierung auch wieder zunichte machen.

b) Implementieren Sie Ihren Entwurf aus a).

c) Überprüfen Sie Ihre Implementation (auch) mit dem untenstehenden Testcode auf Korrektheit, Ihre Funktion sollte (ungefähr, da stochastischer Algorithmus!) das selbe Ergebnis produzieren.  
Überlegen Sie sich weitere Testfälle für Komplikationen die bei der Anwendung auf echte Daten auftauchen könnten und überprüfen Sie Ihre Funktion damit. Schreiben Sie dafür entsprechende `expectations` mit den in `testthat` und/oder `checkmate` implementierten `expect_<BLA>()`-Funktionen.

**Wie immer gilt: Schreiben Sie sauber strukturierten, dokumentierten, kommentierten & funktionalen 
Code der für erwartbare Fehler und ungeeignete Inputs informative Fehlermeldungen und Warnungen produziert.**

-----

#### Testcode:

Wir schreiben also zusätzlich 

- eine Funktion die Daten aus einem linearen Modell mit klaren Outliern 
generiert um Testdatensätze zur Überprüfung unserer Funktion zu erzeugen, sowie
- eine Funktion die die `ransaclm`-Ergebnisse zusammenfasst und für univariate Modelle das
Ergebnis visualisiert.

Das könnte etwa so aussehen:
```{r, ransac_test_utils, code = readLines("ransac-utils.R")}
```
Natürlich können Sie das obenstehende an Ihre Implementation anpassen oder sich auch ähnliche Funktionen selbst basteln.

Ihre Implementation sollte dann -- in etwa -- folgendes Verhalten reproduzieren:
```{r, ransac_example, code = readLines("ransac-example.R")}
```

Hinweis 1: Durch die Formelnotation mit "`- inlier`" bleibt -- in meiner Implementation, zumindest -- die ursprünglich angelegte `inlier`-Variable, welche die wahren *inlier* identifiziert, im Datensatz des Rückgabeobjekts erhalten so dass wir mit `validate_ransac` eine Kreuztabelle der wahren und entdeckten Inlier/Outlier erzeugen können.

Hinweis 2: Es ist zu erwarten dass Parallelisierung hier wenn überhaupt nur bei recht großen Datensätzen Zeitvorteile bringt, in meiner Testimplementation sehe ich zB bei `n_obs = 100000, n_coef = 10` und 20 (!) parallelen Prozessen nur etwa eine Halbierung der Rechenzeit gegenüber einer sequentiellen Berechnung...
